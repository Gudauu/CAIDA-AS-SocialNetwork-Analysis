    if len(y) > 0:
        y = y[index]
    
    model_1.train()
    model_2.train()
    
    bce_total_loss = 0
    skipgram_total_loss = 0
    recon_total_loss = 0
    acc_list, y_list, pred_list = [], [], []
    
    batch_num = int(math.floor(len(edges) / batch_size))
    bar = trange(batch_num, mininterval=0.1, desc='  - (Training) ', leave=False, )
    for i in bar:
        if only_rw or alpha > 0:
            examples, labels, neg_samples, epoch_finished, words = sentences.next_batch()
            examples = torch.tensor(examples, dtype=torch.long, device=device)
            labels = torch.tensor(labels, dtype=torch.long, device=device)
            neg_samples = torch.tensor(neg_samples, dtype=torch.long, device=device)
            loss_skipgram = train_batch_skipgram(
                model_2, loss_2, alpha, [
                    examples, labels, neg_samples])
            loss = loss_skipgram
            acc_list.append(0)
            auc1, auc2 = 0.0, 0.0
            
        else:
            batch_edge = edges[i * batch_size:(i + 1) * batch_size]
            batch_edge_weight = edge_weight[i * batch_size:(i + 1) * batch_size]
            batch_y = ""
            if len(y) > 0:
                batch_y = y[i * batch_size:(i + 1) * batch_size]
                if len(batch_y) == 0:
                    continue
            
            pred, batch_y, loss_bce, loss_recon = train_batch_hyperedge(model_1, loss_1, batch_edge, batch_edge_weight, type, y=batch_y)
            loss_skipgram = torch.Tensor([0.0]).to(device)
            loss = beta * loss_bce + alpha * loss_skipgram + loss_recon * args.rw
            acc_list.append(accuracy(pred, batch_y))
            y_list.append(batch_y)
            pred_list.append(pred)
        
        for opt in optimizer:
            opt.zero_grad()
        
        # backward
        loss.backward()
        
        # update parameters
        for opt in optimizer:
            opt.step()
        
        bar.set_description(" - (Training) BCE:  %.4f  skipgram: %.4f recon: %.4f" %
                            (bce_total_loss / (i + 1), skipgram_total_loss / (i + 1), recon_total_loss / (i + 1)))
        bce_total_loss += loss_bce.item()
        skipgram_total_loss += loss_skipgram.item()
        recon_total_loss += loss_recon.item()
    y = torch.cat(y_list)
    pred = torch.cat(pred_list)
    auc1, auc2 = roc_auc_cuda(y, pred)
    return bce_total_loss / batch_num, skipgram_total_loss / batch_num,recon_total_loss / batch_num, np.mean(acc_list), auc1, auc2


def eval_epoch(args, model, loss_func, validation_data, batch_size, type):
    ''' Epoch operation in evaluation phase '''
    bce_total_loss = 0
    recon_total_loss = 0
    (loss_1, beta), (loss_2, alpha) = loss_func
    
    loss_func = loss_1
    
    model.eval()
    with torch.no_grad():
        validation_data, validation_weight = validation_data
        y = ""
        
        index = torch.randperm(len(validation_data))
        validation_data, validation_weight = validation_data[index], validation_weight[index]
        if len(y) > 0:
            y = y[index]
        
        pred, label = [], []
        
        for i in tqdm(range(int(math.floor(len(validation_data) / batch_size))),
                      mininterval=0.1, desc='  - (Validation)   ', leave=False):
            # prepare data
            batch_x = validation_data[i * batch_size:(i + 1) * batch_size]
            batch_w = validation_weight[i * batch_size:(i + 1) * batch_size]
            
            if len(y) == 0:
                batch_x, batch_y, batch_w = generate_negative(
                    batch_x, "test_dict", type, weight=batch_w)
            else:
                batch_y = y[i * batch_size:(i + 1) * batch_size]
            
            index = torch.randperm(len(batch_x))
            batch_x, batch_y, batch_w = batch_x[index], batch_y[index], batch_w[index]
            
            pred_batch, recon_loss = model(batch_x, return_recon = True)
            pred.append(pred_batch)
            label.append(batch_y)
            
            loss = loss_func(pred_batch, batch_y, weight=batch_w)
            recon_total_loss += recon_loss.item()
            bce_total_loss += loss.item()
        
        pred = torch.cat(pred, dim=0)
        label = torch.cat(label, dim=0)
        
        acc = accuracy(pred, label)
        
        auc1, auc2 = roc_auc_cuda(label, pred)
    
    return bce_total_loss / (i + 1), recon_total_loss / (i + 1), acc, auc1, auc2


def train(args, model, loss, training_data, validation_data, optimizer, epochs, batch_size, only_rw):
    valid_accus = [0]
    # outlier_data = generate_outlier()
    
    for epoch_i in range(epochs):
        if only_rw:
            save_embeddings(model[0], True)
                
                
        
        print('[ Epoch', epoch_i, 'of', epochs, ']')
        
        start = time.time()
        
        bce_loss, skipgram_loss,recon_loss, train_accu, auc1, auc2 = train_epoch(
            args, model, loss, training_data, optimizer, batch_size, only_rw, train_type)
        print('  - (Training)   bce: {bce_loss: 7.4f}, skipgram: {skipgram_loss: 7.4f}, '
              'recon: {recon_loss: 7.4f}'
              ' acc: {accu:3.3f} %, auc: {auc1:3.3f}, aupr: {auc2:3.3f}, '
              'elapse: {elapse:3.3f} s'.format(
            bce_loss=bce_loss,
            skipgram_loss=skipgram_loss,
            recon_loss = recon_loss,
            accu=100 *
                 train_accu,
            auc1=auc1,
            auc2=auc2,
            elapse=(time.time() - start)))
        
        start = time.time()
        valid_bce_loss, recon_loss, valid_accu, valid_auc1, valid_auc2 = eval_epoch(args, model[0], loss, validation_data, batch_size,
                                                                        'hyper')
        print('  - (Validation-hyper) bce: {bce_loss: 7.4f}, recon: {recon_loss: 7.4f},'
              '  acc: {accu:3.3f} %,'
              ' auc: {auc1:3.3f}, aupr: {auc2:3.3f},'
              'elapse: {elapse:3.3f} s'.format(
            bce_loss=valid_bce_loss,
            recon_loss=recon_loss,
            accu=100 *
                 valid_accu,
            auc1=valid_auc1,
            auc2=valid_auc2,
            elapse=(time.time() - start)))
        
        valid_accus += [valid_auc1]
        # check_outlier(model[0], outlier_data)
        
        checkpoint = {
            'model_link': model[0].state_dict(),
            'model_node2vec': model[1].state_dict(),
            'epoch': epoch_i}
        
        model_name = 'model.chkpt'
        
        if valid_auc1 >= max(valid_accus):
            torch.save(checkpoint, os.path.join(args.save_path, model_name))
        
        torch.cuda.empty_cache()
        
    if not only_rw:
        checkpoint = torch.load(os.path.join(args.save_path, model_name))
        model[0].load_state_dict(checkpoint['model_link'])
        model[1].load_state_dict(checkpoint['model_node2vec'])

def generate_negative(x, dict1, get_type='all', weight="", forward=True):
    if dict1 == 'train_dict':
        dict1 = train_dict
    elif dict1 == 'test_dict':
        dict1 = test_dict
    
    if len(weight) == 0:
        weight = torch.ones(len(x), dtype=torch.float)
    
    neg_list = []
    
    zero_num_list = [0] + list(num_list)
    new_index = []
    max_id = int(num[-1])
    
    if forward:
        func1 = pass_
    else:
        func1 = tqdm
    
    if len(x.shape) > 1:
        change_list_all = np.random.randint(
            0, x.shape[-1], len(x) * neg_num).reshape((len(x), neg_num))
    for j, sample in enumerate(func1(x)):
        if len(x.shape) > 1:
            change_list = change_list_all[j, :]
        else:
            change_list = np.random.randint(0, sample.shape[-1], neg_num)
        for i in range(neg_num):
            temp = np.copy(sample)
            a = set()
            a.add(tuple(temp))
            
            trial = 0
            simple_or_hard = np.random.rand()
            if simple_or_hard <= pair_ratio:
                change = change_list[i]
                
            while not a.isdisjoint(dict1):
                temp = np.copy(sample)
                trial += 1
                if trial >= 1000:
                    temp = ""
                    break
                # Only change one node
                if simple_or_hard <= pair_ratio:
                    if len(num_list) == 1:
                        # Only one node type
                        temp[change] = np.random.randint(0, max_id, 1) + 1
                    
                    else:
                        # Multiple node types
                        start = zero_num_list[node_type_mapping[change]]
                        end = zero_num_list[node_type_mapping[change] + 1]
                        
                        temp[change] = np.random.randint(
                            int(start), int(end), 1) + 1
                else:
                    
                    if len(num_list) == 1:
                        # Only one node type
                        temp = np.random.randint(
                            0, max_id, sample.shape[-1]) + 1
                    
                    else:
                        for k in range(temp.shape[-1]):
                            start = zero_num_list[node_type_mapping[k]]
                            end = zero_num_list[node_type_mapping[k] + 1]
                            temp[k] = np.random.randint(
                                int(start), int(end), 1) + 1
                
                temp.sort()
                a = set([tuple(temp)])
            
            if len(temp) > 0:
                neg_list.append(temp)
                if i == 0:
                    new_index.append(j)
    if get_type == 'all' or get_type == 'edge':
        x_e, neg_e = generate_negative_edge(x, int(len(x)))
        if get_type == 'all':
            x = list(x) + x_e
            neg_list = neg_list + neg_e
        else:
            x = x_e
            neg_list = neg_e
    new_index = np.array(new_index)
    new_x = x[new_index]
    
    if not forward:
        device = 'cpu'
    else:
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    
    new_weight = torch.tensor(weight[new_index]).to(device)
    
    x = np2tensor_hyper(new_x, dtype=torch.long)
    neg = np2tensor_hyper(neg_list, dtype=torch.long)
    x = pad_sequence(x, batch_first=True, padding_value=0).to(device)
    neg = pad_sequence(neg, batch_first=True, padding_value=0).to(device)
    # print("x", x, "neg", neg)
    
    return torch.cat([x, neg]), torch.cat(
        [torch.ones((len(x), 1), device=device), torch.zeros((len(neg), 1), device=device)], dim=0), torch.cat(
        ((torch.ones((len(x), 1), device=device) * new_weight.view(-1, 1), (torch.ones((len(neg), 1), device=device)))))


def save_embeddings(model, origin=False):
    model.eval()
    with torch.no_grad():
        ids = np.arange(num_list[-1]) + 1
        ids = torch.Tensor(ids).long().to(device).view(-1, 1)
        embeddings = []
        for j in range(math.ceil(len(ids) / batch_size)):
            x = ids[j * batch_size:min((j + 1) * batch_size, len(ids))]
            if origin:
                embed = model.get_node_embeddings(x)
            else:
                embed = model.get_embedding_static(x)
            embed = embed.detach().cpu().numpy()
