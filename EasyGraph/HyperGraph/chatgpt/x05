

# Note that, no matter how many node types are here, make sure the
# hyperedge (N1,N2,N3,...) has id, N1 < N2 < N3...
train_dict = parallel_build_hash(train_data, "build_hash", args, num, initial = set())
test_dict = parallel_build_hash(test_data, "build_hash", args, num, initial = train_dict)
print ("dict_size", len(train_dict), len(test_dict))

# dict2 = build_hash2(train_data)
# pos_edges = list(dict2)
# pos_edges = np.array(pos_edges)
# np.random.shuffle(pos_edges)

print("train data amount", len(train_data))
# potential_outliers = build_hash3(np.concatenate((train_data, test), axis=0))
# potential_outliers = np.array(list(potential_outliers))


with tf.Graph().as_default(), tf.Session() as session:
    with tf.device("/cpu:0"):
        if args.feature == 'walk':
            # Note that for this part, the word2vec still takes sentences with
            # words starts at "0"
            if not args.TRY and os.path.exists(
                    "../%s_wv_%d_%s.npy" %
                    (args.data, args.dimensions, args.walk)):
                A = np.load(
                    "../%s_wv_%d_%s.npy" %
                    (args.data,
                     args.dimensions,
                     args.walk),
                    allow_pickle=True)
            else:
                print("start loading")
                walks = np.loadtxt(walk_path, delimiter=" ").astype('int')
                start = time.time()
                split_num = 20
                pool = ProcessPoolExecutor(max_workers=split_num)
                process_list = []
                walks = np.array_split(walks, split_num)
                
                result = []
                print("Start turning path to strs")
                for walk in walks:
                    process_list.append(pool.submit(walkpath2str, walk))
                
                for p in as_completed(process_list):
                    result += p.result()
                
                pool.shutdown(wait=True)
                
                walks = result
                print(
                    "Finishing Loading and processing %.2f s" %
                    (time.time() - start))
                print("Start Word2vec")
                import multiprocessing
                
                print("num cpu cores", multiprocessing.cpu_count())
                w2v = Word2Vec(
                    walks,
                    size=args.dimensions,
                    window=args.window_size,
                    min_count=0,
                    sg=1,
                    iter=1,
                    workers=multiprocessing.cpu_count())
                wv = w2v.wv
                A = [wv[str(i)] for i in range(num_list[-1])]
                np.save("../%s_wv_%d_%s.npy" %
                        (args.data, args.dimensions, args.walk), A)
                
                from sklearn.preprocessing import StandardScaler
                
                A = StandardScaler().fit_transform(A)
            
            A = np.concatenate(
                (np.zeros((1, A.shape[-1]), dtype='float32'), A), axis=0)
            A = A.astype('float32')
            A = torch.tensor(A).to(device)
            print(A.shape)
            
            node_embedding = Wrap_Embedding(int(
                num_list[-1] + 1), args.dimensions, scale_grad_by_freq=False, padding_idx=0, sparse=False)
            node_embedding.weight = nn.Parameter(A)
        
        elif args.feature == 'adj':
            flag = False
            
            node_embedding = MultipleEmbedding(
                embeddings_initial,
                bottle_neck,
                flag,
                num_list,
                node_type_mapping).to(device)
        
        classifier_model = Classifier(
            n_head=8,
            d_model=args.dimensions,
            d_k=16,
            d_v=16,
            node_embedding=node_embedding,
            diag_mask=args.diag,
            bottle_neck=bottle_neck).to(device)
        
        save_embeddings(classifier_model, True)
        
        Randomwalk_Word2vec = Word2vec_Skipgram(dict_size=int(num_list[-1] + 1), embedding_dim=args.dimensions,
                                                window_size=args.window_size, u_embedding=node_embedding,
                                                sparse=False).to(device)
        
        loss = F.binary_cross_entropy
        loss2 = torch.nn.BCEWithLogitsLoss(reduction='sum')
        
        summary(classifier_model, (3,))
    
        try:
            from datapipe import Word2Vec_Skipgram_Data
            sentences = Word2Vec_Skipgram_Data(train_data=walk_path,
                                               num_samples=neg_num_w2v,
                                               batch_size=128,
                                               window_size=args.window_size,
                                               min_count=0,
                                               subsample=1e-3,
                                               session=session)
        except:
            sentences = Word2Vec_Skipgram_Data_Empty()
        
        params_list = list(set(list(classifier_model.parameters()) + list(Randomwalk_Word2vec.parameters())))
        
        if args.feature == 'adj':
            optimizer = torch.optim.Adam(params_list, lr=1e-3)
        else:
            optimizer = torch.optim.RMSprop(params_list, lr=1e-3)
        
        model_parameters = filter(lambda p: p.requires_grad, params_list)
        params = sum([np.prod(p.size()) for p in model_parameters])
        print("params to be trained", params)
        
        train(args, (classifier_model, Randomwalk_Word2vec),
              loss=((loss, 1.0), (loss2, 0.0)),
              training_data=(train_data, train_weight, sentences),
              validation_data=(test_data, test_weight),
              optimizer=[optimizer], epochs=300, batch_size=batch_size, only_rw=False)

4. "Modules.py"
import torch.nn as nn
import torch.nn.functional as F
import torch
import numpy as np
from tqdm import tqdm, trange
import copy
import math

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
device_ids = [0, 1]


def get_non_pad_mask(seq):
    assert seq.dim() == 2
    return seq.ne(0).type(torch.float).unsqueeze(-1)


def get_attn_key_pad_mask(seq_k, seq_q):
    ''' For masking out the padding part of key sequence. '''
    
    # Expand to fit the shape of key query attention matrix.
    len_q = seq_q.size(1)
    padding_mask = seq_k.eq(0)
    padding_mask = padding_mask.unsqueeze(
        1).expand(-1, len_q, -1)  # b x lq x lk
    
    return padding_mask


class Wrap_Embedding(torch.nn.Embedding):
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
    
    def forward(self, *input):
        return super().forward(*input), torch.Tensor([0]).to(device)

# Used only for really big adjacency matrix


class SparseEmbedding(nn.Module):
    def __init__(self, embedding_weight, sparse=True):
        super().__init__()
        print(embedding_weight.shape)
        self.sparse = sparse
        if self.sparse:
            self.embedding = embedding_weight
        else:
            try:
                try:
                    self.embedding = torch.from_numpy(
                        np.asarray(embedding_weight.todense())).to(device)
                except BaseException:
                    self.embedding = torch.from_numpy(
                        np.asarray(embedding_weight)).to(device)
            except Exception as e:
                print("Sparse Embedding Error",e)
                self.sparse = True
                self.embedding = embedding_weight
    
    def forward(self, x):
        
        if self.sparse:
            x = x.cpu().numpy()
            x = x.reshape((-1))
            temp = np.asarray((self.embedding[x, :]).todense())
            
            return torch.from_numpy(temp).to(device)
        else:
            return self.embedding[x, :]


class TiedAutoEncoder(nn.Module):
    def __init__(self, inp, out):
        super().__init__()
        self.weight = nn.parameter.Parameter(torch.Tensor(out, inp))
        self.bias1 = nn.parameter.Parameter(torch.Tensor(out))
        self.bias2 = nn.parameter.Parameter(torch.Tensor(inp))
        
        self.register_parameter('tied weight',self.weight)
        self.register_parameter('tied bias1', self.bias1)
        self.register_parameter('tied bias2', self.bias2)
        
        self.reset_parameters()
        
    

    def reset_parameters(self):
        torch.nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))
        if self.bias1 is not None:
            fan_in, _ = torch.nn.init._calculate_fan_in_and_fan_out(self.weight)
            bound = 1 / math.sqrt(fan_in)
            torch.nn.init.uniform_(self.bias1, -bound, bound)
        
        if self.bias2 is not None:
            fan_in, fan_out = torch.nn.init._calculate_fan_in_and_fan_out(self.weight)
            bound = 1 / math.sqrt(fan_out)
            torch.nn.init.uniform_(self.bias2, -bound, bound)

    def forward(self, input):
        encoded_feats = F.linear(input, self.weight, self.bias1)
        encoded_feats = F.tanh(encoded_feats)
        reconstructed_output = F.linear(encoded_feats, self.weight.t(), self.bias2)
        return encoded_feats, reconstructed_output
    
class MultipleEmbedding(nn.Module):
    def __init__(
            self,
            embedding_weights,
            dim,
            sparse=True,
            num_list=None,
            node_type_mapping=None):
        super().__init__()
        print(dim)
        self.num_list = torch.tensor([0] + list(num_list)).to(device)
        print(self.num_list)
        self.node_type_mapping = node_type_mapping
        self.dim = dim
        
        self.embeddings = []
        for i, w in enumerate(embedding_weights):
            try:
                self.embeddings.append(SparseEmbedding(w, sparse))
            except BaseException as e:
                print ("Conv Embedding Mode")
                self.add_module("ConvEmbedding1", w)
                self.embeddings.append(w)
        
        test = torch.zeros(1, device=device).long()
        self.input_size = []
        for w in self.embeddings:
            self.input_size.append(w(test).shape[-1])
        
        self.wstack = [TiedAutoEncoder(self.input_size[i],self.dim).to(device) for i,w in enumerate(self.embeddings)]
        self.norm_stack =[nn.LayerNorm(self.dim).to(device) for w in self.embeddings]
        for i, w in enumerate(self.wstack):
            self.add_module("Embedding_Linear%d" % (i), w)
            self.add_module("Embedding_norm%d" % (i), self.norm_stack[i])
            
        self.dropout = nn.Dropout(0.25)
    
    def forward(self, x):
        
        final = torch.zeros((len(x), self.dim)).to(device)
        recon_loss = torch.Tensor([0.0]).to(device)
        for i in range(len(self.num_list) - 1):
            select = (x >= (self.num_list[i] + 1)) & (x < (self.num_list[i + 1] + 1))
            if torch.sum(select) == 0:
                continue
            adj = self.embeddings[i](x[select] - self.num_list[i] - 1)
            output = self.dropout(adj)
            output, recon = self.wstack[i](output)
            output = self.norm_stack[i](output)
            final[select] = output
