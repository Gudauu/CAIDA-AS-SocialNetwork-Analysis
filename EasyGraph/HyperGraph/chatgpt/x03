import matplotlib as mpl
mpl.use("Agg")
import multiprocessing

cpu_num = multiprocessing.cpu_count()

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
torch.backends.cudnn.benchmark = True
torch.backends.cudnn.deterministic = False
device_ids = [0, 1]
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'

warnings.filterwarnings("ignore")


def parse_args():
    # Parses the node2vec arguments.
    parser = argparse.ArgumentParser(description="Run node2vec.")
    
    parser.add_argument('--data', type=str, default='ramani')
    parser.add_argument('--TRY', action='store_true')
    parser.add_argument('--FILTER', action='store_true')
    parser.add_argument('--grid', type=str, default='')
    parser.add_argument('--remark', type=str, default='')
    
    parser.add_argument('--random-walk', action='store_true')
    
    parser.add_argument('--dimensions', type=int, default=64,
                        help='Number of dimensions. Default is 64.')
    
    parser.add_argument('-l', '--walk-length', type=int, default=40,
                        help='Length of walk per source. Default is 40.')
    
    parser.add_argument('-r', '--num-walks', type=int, default=10,
                        help='Number of walks per source. Default is 10.')
    
    parser.add_argument('-k', '--window-size', type=int, default=10,
                        help='Context size for optimization. Default is 10.')
    
    parser.add_argument('-i', '--iter', default=1, type=int,
                        help='Number of epochs in SGD')
    
    parser.add_argument('--workers', type=int, default=8,
                        help='Number of parallel workers. Default is 8.')
    
    parser.add_argument('--p', type=float, default=2,
                        help='Return hyperparameter. Default is 1.')
    
    parser.add_argument('--q', type=float, default=0.25,
                        help='Inout hyperparameter. Default is 1.')
    
    parser.add_argument('-a', '--alpha', type=float, default=0.0,
                        help='The weight of random walk -skip-gram loss. Default is ')
    parser.add_argument('--rw', type=float, default=0.01,
                        help='The weight of reconstruction of adjacency matrix loss. Default is ')
    parser.add_argument('-w', '--walk', type=str, default='',
                        help='The walk type, empty stands for normal rw')
    parser.add_argument('-d', '--diag', type=str, default='True',
                        help='Use the diag mask or not')
    parser.add_argument(
        '-f',
        '--feature',
        type=str,
        default='walk',
        help='Features used in the first step')
    
    args = parser.parse_args()
    
    if not args.random_walk:
        args.model_name = 'model_no_randomwalk'
        args.epoch = 25
    else:
        args.model_name = 'model_{}_'.format(args.data)
        args.epoch = 25
    if args.TRY:
        args.model_name = 'try' + args.model_name
        if not args.random_walk:
            args.epoch = 5
        else:
            args.epoch = 1
    # args.epoch = 1
    args.model_name += args.remark
    print(args.model_name)
    
    args.save_path = os.path.join(
        '../checkpoints/', args.data, args.model_name)
    if not os.path.exists(args.save_path):
        os.makedirs(args.save_path)
    return args


def train_batch_hyperedge(model, loss_func, batch_data, batch_weight, type, y=""):
    x = batch_data
    w = batch_weight
    
    # When label is not generated, prepare the data
    if len(y) == 0:
        x, y, w = generate_negative(x, "train_dict", type, w)
        index = torch.randperm(len(x))
        x, y, w = x[index], y[index], w[index]
    
    # forward
    pred, recon_loss = model(x, return_recon = True)
    loss = loss_func(pred, y, weight=w)
    return pred, y, loss, recon_loss


def train_batch_skipgram(model, loss_func, alpha, batch_data):
    if alpha == 0:
        return torch.zeros(1).to(device)
    
    examples, labels, neg_samples = batch_data
    
    # Embeddings for examples: [batch_size, emb_dim]
    example_emb = model.forward_u(examples)
    true_w, true_b = model.forward_w_b(labels)
    sampled_w, sampled_b = model.forward_w_b(neg_samples)
    
    # True logits: [batch_size, 1]
    true_logits = torch.sum(torch.mul(example_emb, true_w), dim=1) + true_b
    
    # Sampled logits: [batch_size, num_sampled]
    # We replicate sampled noise labels for all examples in the batch
    # using the matmul.
    sampled_b_vec = sampled_b.view(1, -1)
    
    sampled_logits = torch.matmul(example_emb,
                                  sampled_w.transpose(1, 0))
    sampled_logits += sampled_b_vec
    
    true_xent = loss_func(true_logits, torch.ones_like(true_logits).to(device))
    sampled_xent = loss_func(sampled_logits,
                             torch.zeros_like(sampled_logits).to(device))
    
    # NCE-loss is the sum of the true and noise (sampled words)
    # contributions, averaged over the batch.
    loss = (true_xent + sampled_xent) / len(examples) / len(labels)
    return loss


def train_epoch(args, model, loss_func, training_data, optimizer, batch_size, only_rw, type):
    # Epoch operation in training phase
    # Simultaneously train on 2 models: hyperedge-prediction (1) & random-walk with skipgram (2)
    model_1, model_2 = model
    (loss_1, beta), (loss_2, alpha) = loss_func
    edges, edge_weight, sentences = training_data
    y = torch.tensor([])

    
    # Permutate all the data
    index = torch.randperm(len(edges))
    edges, edge_weight = edges[index], edge_weight[index]
    if len(y) > 0:
        y = y[index]
    
    model_1.train()
    model_2.train()
    
    bce_total_loss = 0
    skipgram_total_loss = 0
    recon_total_loss = 0
    acc_list, y_list, pred_list = [], [], []
    
    batch_num = int(math.floor(len(edges) / batch_size))
    bar = trange(batch_num, mininterval=0.1, desc='  - (Training) ', leave=False, )
    for i in bar:
        if only_rw or alpha > 0:
            examples, labels, neg_samples, epoch_finished, words = sentences.next_batch()
            examples = torch.tensor(examples, dtype=torch.long, device=device)
            labels = torch.tensor(labels, dtype=torch.long, device=device)
            neg_samples = torch.tensor(neg_samples, dtype=torch.long, device=device)
            loss_skipgram = train_batch_skipgram(
                model_2, loss_2, alpha, [
                    examples, labels, neg_samples])
            loss = loss_skipgram
            acc_list.append(0)
            auc1, auc2 = 0.0, 0.0
            
        else:
            batch_edge = edges[i * batch_size:(i + 1) * batch_size]
            batch_edge_weight = edge_weight[i * batch_size:(i + 1) * batch_size]
            batch_y = ""
            if len(y) > 0:
                batch_y = y[i * batch_size:(i + 1) * batch_size]
                if len(batch_y) == 0:
                    continue
            
            pred, batch_y, loss_bce, loss_recon = train_batch_hyperedge(model_1, loss_1, batch_edge, batch_edge_weight, type, y=batch_y)
            loss_skipgram = torch.Tensor([0.0]).to(device)
            loss = beta * loss_bce + alpha * loss_skipgram + loss_recon * args.rw
            acc_list.append(accuracy(pred, batch_y))
            y_list.append(batch_y)
            pred_list.append(pred)
        
        for opt in optimizer:
            opt.zero_grad()
        
        # backward
        loss.backward()
        
        # update parameters
        for opt in optimizer:
            opt.step()
        
        bar.set_description(" - (Training) BCE:  %.4f  skipgram: %.4f recon: %.4f" %
                            (bce_total_loss / (i + 1), skipgram_total_loss / (i + 1), recon_total_loss / (i + 1)))
        bce_total_loss += loss_bce.item()
        skipgram_total_loss += loss_skipgram.item()
        recon_total_loss += loss_recon.item()
    y = torch.cat(y_list)
    pred = torch.cat(pred_list)
    auc1, auc2 = roc_auc_cuda(y, pred)
    return bce_total_loss / batch_num, skipgram_total_loss / batch_num,recon_total_loss / batch_num, np.mean(acc_list), auc1, auc2


def eval_epoch(args, model, loss_func, validation_data, batch_size, type):
    ''' Epoch operation in evaluation phase '''
    bce_total_loss = 0
    recon_total_loss = 0
    (loss_1, beta), (loss_2, alpha) = loss_func
    
    loss_func = loss_1
    
    model.eval()
    with torch.no_grad():
        validation_data, validation_weight = validation_data
        y = ""
        
        index = torch.randperm(len(validation_data))
        validation_data, validation_weight = validation_data[index], validation_weight[index]
        if len(y) > 0:
            y = y[index]
        
        pred, label = [], []
        
        for i in tqdm(range(int(math.floor(len(validation_data) / batch_size))),
                      mininterval=0.1, desc='  - (Validation)   ', leave=False):
            # prepare data
            batch_x = validation_data[i * batch_size:(i + 1) * batch_size]
            batch_w = validation_weight[i * batch_size:(i + 1) * batch_size]
            
            if len(y) == 0:
                batch_x, batch_y, batch_w = generate_negative(
                    batch_x, "test_dict", type, weight=batch_w)
            else:
                batch_y = y[i * batch_size:(i + 1) * batch_size]
            
            index = torch.randperm(len(batch_x))
            batch_x, batch_y, batch_w = batch_x[index], batch_y[index], batch_w[index]
            
            pred_batch, recon_loss = model(batch_x, return_recon = True)
            pred.append(pred_batch)
            label.append(batch_y)
            
            loss = loss_func(pred_batch, batch_y, weight=batch_w)
            recon_total_loss += recon_loss.item()
            bce_total_loss += loss.item()
        
        pred = torch.cat(pred, dim=0)
        label = torch.cat(label, dim=0)
        
        acc = accuracy(pred, label)
        
        auc1, auc2 = roc_auc_cuda(label, pred)
    
    return bce_total_loss / (i + 1), recon_total_loss / (i + 1), acc, auc1, auc2


def train(args, model, loss, training_data, validation_data, optimizer, epochs, batch_size, only_rw):
    valid_accus = [0]
    # outlier_data = generate_outlier()
    
    for epoch_i in range(epochs):
        if only_rw:
            save_embeddings(model[0], True)
                
                
        
        print('[ Epoch', epoch_i, 'of', epochs, ']')
        
        start = time.time()
        
        bce_loss, skipgram_loss,recon_loss, train_accu, auc1, auc2 = train_epoch(
            args, model, loss, training_data, optimizer, batch_size, only_rw, train_type)
        print('  - (Training)   bce: {bce_loss: 7.4f}, skipgram: {skipgram_loss: 7.4f}, '
              'recon: {recon_loss: 7.4f}'
              ' acc: {accu:3.3f} %, auc: {auc1:3.3f}, aupr: {auc2:3.3f}, '
              'elapse: {elapse:3.3f} s'.format(
            bce_loss=bce_loss,
            skipgram_loss=skipgram_loss,
            recon_loss = recon_loss,
            accu=100 *
                 train_accu,
            auc1=auc1,
            auc2=auc2,
            elapse=(time.time() - start)))
        
        start = time.time()
        valid_bce_loss, recon_loss, valid_accu, valid_auc1, valid_auc2 = eval_epoch(args, model[0], loss, validation_data, batch_size,
                                                                        'hyper')
